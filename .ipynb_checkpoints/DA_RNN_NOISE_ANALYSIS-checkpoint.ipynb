{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of Gaussian noise addition on the prediction performance of the Dual Attention Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "06/2021 Jonathan Fiorentino & Michele Monti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import collections\n",
    "import typing\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as tf\n",
    "import re\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import typing\n",
    "from typing import Tuple\n",
    "import json\n",
    "\n",
    "from torch import optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from My_allFunctions import*\n",
    "\n",
    "import time\n",
    "\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction performance on the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileListtxt=[]\n",
    "FileListcsv=[]\n",
    "subList=[]\n",
    "\n",
    "# Get the names of the subfolders in DATA and the file with the protein time traces\n",
    "subfolders=[x[0] for x in os.walk('./DATA/')][1:]\n",
    "\n",
    "for sub in subfolders:\n",
    "    print('----------- %s ------------' % (sub))\n",
    "    for file in os.listdir(sub):\n",
    "        if 'Protein' in file and 'txt' in file:\n",
    "            print(file)\n",
    "            subList.append(sub)\n",
    "            FileListtxt.append(sub+'/'+file)\n",
    "            FileListcsv.append(sub+'/'+file.replace('txt','csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_rnn_kwargs = {\"batch_size\": 128, \"T\": 50}\n",
    "\n",
    "for i in range(len(FileListtxt)):\n",
    "    \n",
    "    a=time.time()\n",
    "    txtFile=FileListtxt[i]\n",
    "    csvFile=FileListcsv[i]\n",
    "    sub=subList[i]\n",
    "    \n",
    "    timelist = np.loadtxt(txtFile)[:,0]\n",
    "    dt = timelist[3]-timelist[2]\n",
    "\n",
    "    _, cols = txt_to_csv(txtFile, csvFile, ncols = [0])\n",
    "    origmat=np.zeros((len(timelist),len(cols)+1))\n",
    "    predmat=np.zeros((len(timelist),len(cols)+1))\n",
    "    predmat[:,0]=timelist\n",
    "    origmat[:,0]=timelist\n",
    "    \n",
    "    j=0\n",
    "    for target in cols:\n",
    "        print(target)\n",
    "        a=time.time()\n",
    "        target=[target,]\n",
    "        raw_data = pd.read_csv(csvFile, nrows=5000, usecols = cols)\n",
    "        \n",
    "        # Data preprocessing\n",
    "        prep,scaler= my_preprocess_data(raw_data, target)\n",
    "        \n",
    "        # Load the trained network\n",
    "        netFolder=txtFile.replace('.txt','')+'/'+target[0]+'/'\n",
    "             \n",
    "        with open(netFolder+\"enc_kwargs.json\", \"r\") as fi:\n",
    "            enc_kwargs = json.load(fi)\n",
    "        encoder = Encoder(**enc_kwargs)\n",
    "        encoder.load_state_dict(torch.load(netFolder+\"encoder.torch\", map_location=device))\n",
    "        \n",
    "        with open(netFolder+\"dec_kwargs.json\", \"r\") as fi:\n",
    "            dec_kwargs = json.load(fi)\n",
    "        dec_kwargs['out_feats']=1    \n",
    "        decoder = Decoder(**dec_kwargs)\n",
    "        decoder.load_state_dict(torch.load(netFolder+\"decoder.torch\", map_location=device))\n",
    "        \n",
    "        y_pred_orig,mse_orig,mape_orig=my_noisy_predict(encoder,decoder, prep, 50, 1500,0.)\n",
    "        predmat[:,j+1]=y_pred_orig[:,0]\n",
    "        origmat[:,j+1]=prep.targs[:,0]\n",
    "        j+=1\n",
    "#     np.savetxt('pred_'+re.sub(r'^.*?Protein', 'Protein', txtFile),np.c_[predmat],fmt='%f',delimiter='\\t')\n",
    "#     np.savetxt('orig_'+re.sub(r'^.*?Protein', 'Protein', txtFile),np.c_[origmat],fmt='%f',delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addition of Gaussian noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_noisy_predict(enc,dec, prep: TrainData, T: int, TimeFuture: int,sig):\n",
    "    \n",
    "    if TimeFuture+T > prep.targs.shape[0]:\n",
    "        TimeFuture=prep.targs.shape[0]-T\n",
    "    \n",
    "    batch_size= 1\n",
    "    da_rnn_kwargs = {\"batch_size\": 1, \"T\": T}\n",
    "\n",
    "    out_size = prep.targs.shape[1]\n",
    "    y_pred = np.zeros((TimeFuture+T, out_size))\n",
    "    y_pred[range(T)]= prep.targs[range(T)]\n",
    "\n",
    "    for y_i in range(T, T+TimeFuture, batch_size):\n",
    "\n",
    "        y_slc = slice(y_i, y_i + batch_size)\n",
    "        batch_idx = range(T+TimeFuture)[y_slc]\n",
    "        #takes all the value for each batch size of the data\n",
    "        b_len = len(batch_idx)\n",
    "\n",
    "        X = np.zeros((b_len, T - 1, prep.feats.shape[1]))\n",
    "        y_history = np.zeros((b_len, T - 1, prep.targs.shape[1]))\n",
    "\n",
    "\n",
    "        for b_i, b_idx in enumerate(batch_idx):\n",
    "            idx = range(b_idx - T, b_idx - 1)\n",
    "            X[b_i, :, :] = prep.feats[idx,:]\n",
    "            y_history[b_i, :] = y_pred[idx]\n",
    "\n",
    "        y_history=y_history+np.random.normal(0,sig,np.shape(y_history))\n",
    "        print(np.shape(y_history))\n",
    "        y_history = numpy_to_tvar(y_history)\n",
    "        att,_, input_encoded = enc(numpy_to_tvar(X))\n",
    "\n",
    "        y_pred[y_slc] = dec(input_encoded, y_history).cpu().data.numpy()\n",
    "\n",
    "    # Compute the mean square error of the prediction\n",
    "    mse = np.mean((prep.targs - y_pred)**2)\n",
    "    for i in range(prep.targs.shape[1]):\n",
    "        mape=sklearn.metrics.mean_absolute_percentage_error(prep.targs[:,i],y_pred[:,i])\n",
    "    \n",
    "    return y_pred,mse,mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "da_rnn_kwargs = {\"batch_size\": 128, \"T\": 50}\n",
    "\n",
    "mseTotMatrices=[]\n",
    "mapeTotMatrices=[]\n",
    "\n",
    "for i in range(len(FileListtxt)):\n",
    "    \n",
    "    a=time.time()\n",
    "    txtFile=FileListtxt[i]\n",
    "    csvFile=FileListcsv[i]\n",
    "    sub=subList[i]\n",
    "    print(i,txtFile)\n",
    "    \n",
    "    timelist = np.loadtxt(txtFile)[:,0]\n",
    "    dt = timelist[3]-timelist[2]\n",
    "\n",
    "    _, cols = txt_to_csv(txtFile, csvFile, ncols = [0])\n",
    "    \n",
    "    mseMat=np.zeros((len(cols),1+len(np.arange(.1,1.,.1))))\n",
    "    mapeMat=np.zeros((len(cols),1+len(np.arange(.1,1.,.1))))\n",
    "    \n",
    "    j=0\n",
    "    for target in cols:\n",
    "        a=time.time()\n",
    "        target=[target,]\n",
    "        raw_data = pd.read_csv(csvFile, nrows=5000, usecols = cols)\n",
    "        \n",
    "        # Data preprocessing\n",
    "        prep,scaler= my_preprocess_data(raw_data, target)\n",
    "        \n",
    "        # Load the trained network\n",
    "        netFolder=txtFile.replace('.txt','')+'/'+target[0]+'/'\n",
    "        \n",
    "        with open(netFolder+\"enc_kwargs.json\", \"r\") as fi:\n",
    "            enc_kwargs = json.load(fi)\n",
    "        encoder = Encoder(**enc_kwargs)\n",
    "        encoder.load_state_dict(torch.load(netFolder+\"encoder.torch\", map_location=device))\n",
    "        \n",
    "        with open(netFolder+\"dec_kwargs.json\", \"r\") as fi:\n",
    "            dec_kwargs = json.load(fi)\n",
    "        dec_kwargs['out_feats']=1    \n",
    "        decoder = Decoder(**dec_kwargs)\n",
    "        decoder.load_state_dict(torch.load(netFolder+\"decoder.torch\", map_location=device))\n",
    "        \n",
    "        y_pred_orig,mse_orig,mape_orig=my_noisy_predict(encoder,decoder, prep, 50, 1500,0.)\n",
    "\n",
    "        mse_vec=[]\n",
    "        mape_vec=[]\n",
    "        \n",
    "        mse_vec.append(mse_orig)\n",
    "        mape_vec.append(mape_orig)\n",
    "        \n",
    "        # Add noise with increasing amplitude and predict the gene expression \n",
    "        for sigma in np.arange(.1,1.,.1):\n",
    "            y_pred,mse,mape=my_noisy_predict(encoder,decoder, prep, 50, 1500,sigma)\n",
    "            mse_vec.append(mse)\n",
    "            mape_vec.append(mape)\n",
    "        \n",
    "        mseMat[j,:]=np.array(mse_vec)\n",
    "        mapeMat[j,:]=np.array(mape_vec)\n",
    "        \n",
    "        b=time.time()   \n",
    "        print(b-a)\n",
    "        j+=1\n",
    "    att_matt_dir='./DATA/Noise_analysis/'\n",
    "    if os.path.isdir(att_matt_dir)==False:\n",
    "        os.mkdir(att_matt_dir)\n",
    "    np.savetxt(att_matt_dir+'mse_mat'+sub.replace('/','_')+re.sub(r'^.*?Protein', 'Protein', txtFile),np.c_[mseMat],fmt='%f',delimiter='\\t')\n",
    "    np.savetxt(att_matt_dir+'mape_mat'+sub.replace('/','_')+re.sub(r'^.*?Protein', 'Protein', txtFile),np.c_[mapeMat],fmt='%f',delimiter='\\t')\n",
    "        \n",
    "    mseTotMatrices.append(mseMat)\n",
    "    mapeTotMatrices.append(mapeMat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hopreg-venv",
   "language": "python",
   "name": "hopreg-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
