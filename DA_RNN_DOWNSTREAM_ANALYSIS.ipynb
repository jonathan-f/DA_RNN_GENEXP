{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the attention matrices of Dual Attention Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "06/2021 Jonathan Fiorentino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import antropy as ant\n",
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data for each gene regulatory network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noiseFolder='./DATA/Noise_analysis/'\n",
    "subfolders=[x[0] for x in os.walk('./DATA/')]\n",
    "\n",
    "stringList=['FullyConnected','FullyRepressed','MasterRegulator','mediumConnection','SparseConnection',\n",
    "         'Oscillating','ExternalSignal']\n",
    "\n",
    "netList=[]\n",
    "mseMatList=[]\n",
    "mapeMatList=[]\n",
    "zeroGenesList=[]\n",
    "permEntlist=[]\n",
    "\n",
    "for string in stringList:\n",
    "    subfold = [s for s in subfolders if string in s]\n",
    "    files=[s for s in os.listdir(noiseFolder) if string in s and 'txt' in s and ('mse' in s or 'mape' in s)]\n",
    "    if len(files)==2:\n",
    "        timeseries=[f for f in os.listdir(subfold[0]) if 'Protein' in f and 'txt' in f][0]\n",
    "        timeseriesdata=np.loadtxt(subfold[0]+'/'+timeseries)\n",
    "        timeseriesdata=timeseriesdata[:,1:]\n",
    "        permentonenet=[]\n",
    "        for i in range(timeseriesdata.shape[1]):\n",
    "            perment=ant.perm_entropy(timeseriesdata[:,i], normalize=True)\n",
    "            permentonenet.append(perment)\n",
    "        permEntlist.append(permentonenet)\n",
    "        zeroGenesList.append(np.where(np.all(timeseriesdata==0,axis=0))[0])\n",
    "        netList.append(string)\n",
    "        for file in files:\n",
    "            if 'mse' in file:\n",
    "                mseMat=np.loadtxt(noiseFolder+file)\n",
    "                mseMatList.append(mseMat)\n",
    "            if 'mape' in file:\n",
    "                mapeMat=np.loadtxt(noiseFolder+file)\n",
    "                mapeMatList.append(mapeMat)\n",
    "    else:\n",
    "        tmpfiles=[s for s in files if 'mse' in s]\n",
    "        substrings=[re.sub(r'^.*?nr', 'nr', s) for s in tmpfiles]\n",
    "        substrings=[s.replace('.txt','') for s in substrings]\n",
    "        numbers=[int(re.sub(r'^.*?na', '', s)) for s in substrings]\n",
    "        numbers_sorted=sorted(range(len(numbers)), key=lambda k: numbers[k])\n",
    "        substrings=[substrings[n] for n in numbers_sorted]\n",
    "        for sub in substrings:\n",
    "            timeseries=[f for f in os.listdir(subfold[0]) if 'Protein' in f and 'txt' in f and sub in f][0]\n",
    "            timeseriesdata=np.loadtxt(subfold[0]+'/'+timeseries)\n",
    "            timeseriesdata=timeseriesdata[:,1:]\n",
    "            permentonenet=[]\n",
    "            for i in range(timeseriesdata.shape[1]):\n",
    "                perment=ant.perm_entropy(timeseriesdata[:,i], normalize=True)\n",
    "                permentonenet.append(perment)\n",
    "                permEntlist.append(permentonenet)\n",
    "            zeroGenesList.append(np.where(np.all(timeseriesdata==0,axis=0))[0])\n",
    "            netList.append(string+'_'+sub)\n",
    "            newfiles=[s for s in files if sub in s]\n",
    "            for file in newfiles:\n",
    "                if 'mse' in file:\n",
    "                    mseMat=np.loadtxt(noiseFolder+file)\n",
    "                    mseMatList.append(mseMat)\n",
    "                if 'mape' in file:\n",
    "                    mapeMat=np.loadtxt(noiseFolder+file)\n",
    "                    mapeMatList.append(mapeMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each network, build dataframes of the MSE and MAPE for each gene and each noise level\n",
    "# Compute the mean and the CV of MSE and MAPE over the different noise levels\n",
    "# Remove genes that are never expressed\n",
    "# Remove genes for which the MSE of the original prediction is zero (those are genes that are expressed only in\n",
    "# a subset of the time window used for training and zero elsewhere)\n",
    "import scipy\n",
    "def todf(arr):\n",
    "    df=pd.DataFrame(data=arr)\n",
    "    df.index=['Gene_'+str(i) for i in range(20)]\n",
    "    \n",
    "    return df;\n",
    "\n",
    "msedfList=[]\n",
    "mapedfList=[]\n",
    "rankedmeanMSElist=[]\n",
    "rankedcvMSElist=[]\n",
    "rankedmeanMAPElist=[]\n",
    "rankedcvMAPElist=[]\n",
    "i=0\n",
    "\n",
    "NetNoiseMat=np.zeros((len(netList),40))\n",
    "\n",
    "for (mat1,mat2,zg) in zip(mseMatList,mapeMatList,zeroGenesList):\n",
    "    print(netList[i])\n",
    "    mydf=todf(mat1)\n",
    "    if len(zg)>0:\n",
    "        genes_to_delete=['Gene_'+str(j) for j in zg]\n",
    "        mydf=mydf.drop(genes_to_delete)\n",
    "    \n",
    "    mydf=mydf[mydf[mydf.columns[0]] != 0]\n",
    "    mydf['meanMSE']=todf(mat1).mean(axis=1)\n",
    "    mydf['cvMSE']=todf(mat1).std(axis=1)/todf(mat1).mean(axis=1)\n",
    "    \n",
    "    # Uncomment this for independent sorting of means and coefficient of variations\n",
    "#     meanlist=sorted(list(todf(mat1).mean(axis=1)))\n",
    "#     meanlist=[x/max(meanlist) for x in meanlist]\n",
    "#     varlist=sorted(list(todf(mat1).std(axis=1)/todf(mat1).mean(axis=1)))\n",
    "#     varlist=[0 if np.isnan(x) else x for x in varlist]\n",
    "    \n",
    "    # CV normalization\n",
    "#     varlist=[x/max(varlist) for x in varlist]\n",
    "    \n",
    "    # Uncomment this for dependent sorting of means and coefficient of variations\n",
    "    meanlist=list(todf(mat1).mean(axis=1))\n",
    "    indices_sorted=sorted(range(len(meanlist)), key=lambda k: meanlist[k])\n",
    "    meanlist=[meanlist[k] for k in indices_sorted]\n",
    "    meanlist=[x/max(meanlist) for x in meanlist]\n",
    "#     varlist=list(todf(mat1).std(axis=1)/todf(mat1).mean(axis=1))\n",
    "    varlist=list(todf(mat1).var(axis=1))\n",
    "    varlist=[varlist[k] for k in indices_sorted]\n",
    "    varlist=[0 if np.isnan(x) else x for x in varlist]\n",
    "    \n",
    "    # CV normalization\n",
    "    varlist=[x/max(varlist) for x in varlist]\n",
    "    \n",
    "    if len(meanlist)<20:\n",
    "        meanlist=[0.]*(20-len(meanlist))+meanlist\n",
    "        varlist=[0.]*(20-len(varlist))+varlist\n",
    "    NetNoiseMat[i]=np.array(meanlist+varlist)\n",
    "    print(len(meanlist),len(varlist))\n",
    "#     print(scipy.stats.spearmanr(mydf['meanMSE'],mydf['cvMSE']))\n",
    "    msedfList.append(mydf)\n",
    "    rankedmeanMSEdf=mydf.sort_values('meanMSE')\n",
    "    rankedcvMSEdf=mydf.sort_values('cvMSE')\n",
    "    rankedmeanMSElist.append(rankedmeanMSEdf)\n",
    "    rankedcvMSElist.append(rankedcvMSEdf)\n",
    "    mydf2=todf(mat2)\n",
    "    if len(zg)>0:\n",
    "        genes_to_delete=['Gene_'+str(j) for j in zg]\n",
    "        mydf2=mydf2.drop(genes_to_delete)\n",
    "    mydf2=mydf2[mydf2[mydf2.columns[0]] != 0]\n",
    "    mydf2['meanMAPE']=todf(mat2).mean(axis=1)\n",
    "    mydf2['cvMAPE']=todf(mat2).std(axis=1)/todf(mat2).mean(axis=1)\n",
    "#     print(scipy.stats.spearmanr(mydf2['meanMAPE'],mydf2['cvMAPE']))\n",
    "    mapedfList.append(mydf2)\n",
    "    rankedmeanMAPEdf=mydf2.sort_values('meanMAPE')\n",
    "    rankedcvMAPEdf=mydf2.sort_values('cvMAPE')\n",
    "    rankedmeanMAPElist.append(rankedmeanMAPEdf)\n",
    "    rankedcvMAPElist.append(rankedcvMAPEdf)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering the matrix of mean and variances of the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.pyplot import cm\n",
    "from scipy.cluster import hierarchy\n",
    "import matplotlib as mpl\n",
    "import rpy2.robjects.numpy2ri\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects.packages import importr\n",
    "\n",
    "\n",
    "mycolors=['C0','C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12']\n",
    "\n",
    "markers=['o', 'v', '^', '<', '>', '8', 's', 'p', '*', 'h', 'H', 'D']\n",
    "\n",
    "def getNewick(node, newick, parentdist, leaf_names):\n",
    "    if node.is_leaf():\n",
    "        return \"%s:%.2f%s\" % (leaf_names[node.id], parentdist - node.dist, newick)\n",
    "    else:\n",
    "        if len(newick) > 0:\n",
    "            newick = \"):%.2f%s\" % (parentdist - node.dist, newick)\n",
    "        else:\n",
    "            newick = \");\"\n",
    "        newick = getNewick(node.get_left(), newick, node.dist, leaf_names)\n",
    "        newick = getNewick(node.get_right(), \",%s\" % (newick), node.dist, leaf_names)\n",
    "        newick = \"(%s\" % (newick)\n",
    "        return newick\n",
    "\n",
    "# Function that plots a dendrogram\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "\n",
    "    # Children of hierarchical clustering\n",
    "    children = model.children_\n",
    "\n",
    "    # Distances between each pair of children\n",
    "    # Since we don't have this information, we can use a uniform one for plotting\n",
    "    distance = np.arange(children.shape[0])\n",
    "\n",
    "    # The number of observations contained in each cluster level\n",
    "    no_of_observations = np.arange(2, children.shape[0]+2)\n",
    "\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "    linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)\n",
    "    \n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs,color_threshold=0)\n",
    "\n",
    "def PlotResults(model,netList,mydf,label,save=False):\n",
    "    \n",
    "    fig=plt.figure(figsize=(10,10))\n",
    "    fig.suptitle(label)\n",
    "    gs=GridSpec(2,2) # 2 rows, 2 columns\n",
    "    \n",
    "    ax1=fig.add_subplot(gs[0,:]) # Second row, span all columns\n",
    "    ax2=fig.add_subplot(gs[1,0]) # First row, first column\n",
    "    ax3=fig.add_subplot(gs[1,1]) # First row, second column\n",
    "    ax1.text(-0.1, 1.15, 'A', transform=ax1.transAxes,\n",
    "      fontsize=20, fontweight='bold', va='top', ha='right')\n",
    "    ax2.text(-0.1, 1.15, 'B', transform=ax2.transAxes,\n",
    "      fontsize=20, fontweight='bold', va='top', ha='right')\n",
    "    ax3.text(-0.1, 1.15, 'C', transform=ax3.transAxes,\n",
    "      fontsize=20, fontweight='bold', va='top', ha='right')\n",
    "    # Plot the dendrogram\n",
    "    \n",
    "    plot_dendrogram(model, labels=netList,leaf_rotation=90,ax=ax1)\n",
    "    sns.scatterplot(x='principal component 1',\n",
    "                y='principal component 2',s=100,\n",
    "                data=mydf,hue='networks',style='networks',markers=markers,ax=ax2)\n",
    "    ax2.legend(frameon=False,bbox_to_anchor=(1,1),title='Networks')\n",
    "    \n",
    "    sns.scatterplot(x='principal component 1',\n",
    "                y='principal component 2',s=100,\n",
    "                data=mydf,hue='clusters',style='networks',markers=markers,palette='deep',ax=ax3)\n",
    "    N=model.n_clusters+1\n",
    "    handles, labels = ax3.get_legend_handles_labels()\n",
    "    handles=handles[:N]\n",
    "    labels=labels[:N]\n",
    "    ax3.legend(handles, labels,frameon=True,loc=0)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    if save==True:\n",
    "        plt.savefig('Clustering_'+label+'.pdf',bbox_inches='tight'),plt.close()\n",
    "    else:\n",
    "        plt.show(),plt.close()\n",
    "\n",
    "\n",
    "# Determine optimal number of clusters\n",
    "def optimalClusters(X):\n",
    "\n",
    "    sil_score_max = -1 #this is the minimum possible score\n",
    "    for n_clusters in range(2,10):\n",
    "        model = AgglomerativeClustering(n_clusters=n_clusters, affinity='euclidean', linkage='complete')\n",
    "        labels = model.fit_predict(X)\n",
    "        sil_score = silhouette_score(X, labels)\n",
    "\n",
    "        print(\"The average silhouette score for %i clusters is %0.2f\" %(n_clusters,sil_score))\n",
    "        if sil_score > sil_score_max:\n",
    "            sil_score_max = sil_score\n",
    "            best_n_clusters = n_clusters\n",
    "    return best_n_clusters;\n",
    "\n",
    "# Function that takes a matrix, performs hierarchical clustering\n",
    "# and chooses the optimal number of clusters\n",
    "# Then it plots a dendrogram and a PCA plot with the nets and the results of the clustering\n",
    "def Cluster_and_PCA(mat,netList, label, scale=False, save=False):\n",
    "    if scale==True:\n",
    "        x = StandardScaler().fit_transform(mat)\n",
    "    else:\n",
    "        x=mat\n",
    "    # Principal Components Analysis\n",
    "    pca = PCA(n_components=2)\n",
    "    principalComponents = pca.fit_transform(x)\n",
    "    principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "    \n",
    "    \n",
    "    # Find optimal number of clusters\n",
    "    best_n_clusters = optimalClusters(x)\n",
    "    # Hierarchical clustering\n",
    "    cluster = AgglomerativeClustering(n_clusters=best_n_clusters, affinity='euclidean', linkage='complete')\n",
    "        \n",
    "    \n",
    "    \n",
    "    myclusters=cluster.fit_predict(x)\n",
    "    principalDf['networks']=netList\n",
    "    principalDf['clusters']=myclusters\n",
    "    model = cluster.fit(x)\n",
    "\n",
    "    PlotResults(model,netList,principalDf,label,save=save)\n",
    "    \n",
    "    Z=linkage(x, method='complete', metric='euclidean', optimal_ordering=False)\n",
    "    leaf_names=dendrogram(Z, labels=netList,leaf_rotation=90,color_threshold=0)['ivl']\n",
    "    tree = hierarchy.to_tree(Z)\n",
    "    \n",
    "    newick_tree=getNewick(tree, \"\", tree.dist, leaf_names)\n",
    "    \n",
    "    return newick_tree,model.labels_;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NetNoiseMatdf=pd.DataFrame(data=NetNoiseMat[:,:20])\n",
    "NetNoiseMatdf.index=netList\n",
    "NetNoiseMatdf=NetNoiseMatdf[NetNoiseMatdf.columns[::-1]]\n",
    "NetNoiseMatdf.to_csv('noisematrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelList=[]\n",
    "treeList=[]\n",
    "clustersList=[]\n",
    "\n",
    "noisetree,labnoise=Cluster_and_PCA(NetNoiseMat,netList, 'Noise_analysis', scale=True,save=False)\n",
    "\n",
    "treeList.append(noisetree)\n",
    "clustersList.append(labnoise)\n",
    "labelList.append('Noise_analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = StandardScaler().fit_transform(NetNoiseMat)\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "    \n",
    "    \n",
    "# Find optimal number of clusters\n",
    "best_n_clusters = optimalClusters(x)\n",
    "# Hierarchical clustering\n",
    "cluster = AgglomerativeClustering(n_clusters=best_n_clusters, affinity='euclidean', linkage='complete')\n",
    "myclusters=cluster.fit_predict(x)\n",
    "principalDf['networks']=netList\n",
    "principalDf['clusters']=myclusters\n",
    "model = cluster.fit(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newindex=['ExternalSignal_nr10_na10', 'ExternalSignal_nr1_na1',\n",
    "       'ExternalSignal_nr5_na5', 'FullyConnected',\n",
    "       'FullyRepressed',\n",
    "       'Oscillating_nr10_na10',\n",
    "       'Oscillating_nr15_na15',\n",
    "       'Oscillating_nr1_na1',\n",
    "       'Oscillating_nr5_na5',\n",
    "       'SparseConnection', 'mediumConnection',\n",
    "       'MasterRegulator']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering the graph descriptors matrices of the attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir('./DATA/Matrici_Edo/'):\n",
    "    mydf=pd.read_csv('./DATA/Matrici_Edo/'+file,skiprows=1,index_col=0,header=None)\n",
    "    mydf.index=newindex\n",
    "    mydf=mydf.reindex(netList)\n",
    "    \n",
    "    label=file.replace('.csv','')\n",
    "    label=label.replace('df_','')\n",
    "    print(file,label)\n",
    "    mydf.to_csv('matrix_'+label+'.csv')\n",
    "    d,labs=Cluster_and_PCA(mydf,list(mydf.index), label, scale=True,save=False)\n",
    "    treeList.append(d)\n",
    "    labelList.append(label)\n",
    "    clustersList.append(labs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downstream comparisons between the clusterings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(treeList)):\n",
    "    print(labelList[i])\n",
    "    text_file = open(labelList[i]+'_tree.txt', 'w')\n",
    "    #write string to file\n",
    "    text_file.write(treeList[i]) \n",
    "    #close file\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ete3 import Tree\n",
    "\n",
    "VImatrix=np.zeros((len(treeList),len(treeList)))\n",
    "\n",
    "for i in range(len(treeList)):\n",
    "    for j in range(len(treeList)):\n",
    "        \n",
    "        cl1 = igraph.clustering.Clustering(clustersList[i])\n",
    "        cl2=igraph.clustering.Clustering(clustersList[j])\n",
    "        \n",
    "        VImatrix[i,j]=igraph.compare_communities(cl1, cl2, method='vi', remove_none=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=[0,1,3,7]\n",
    "\n",
    "VImatrixnew=VImatrix[indices,:]\n",
    "VImatrixnew=VImatrixnew[:,indices]\n",
    "\n",
    "labelListnew=['Noise_analysis',\n",
    " 'HubScore','Betweenness','Clustering_Coefficient']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "mask = np.triu(np.ones_like(VImatrixnew, dtype=bool))\n",
    "\n",
    "sns.heatmap(VImatrixnew,cmap='viridis',xticklabels=labelListnew,yticklabels=labelListnew,\n",
    "           cbar_kws={'label': 'Variation of information (bits)'},mask=mask,annot=True,ax=ax)\n",
    "\n",
    "plt.savefig('vimatrixnew.pdf',bbox_inches='tight'),plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendromat=pd.read_csv('dendrodist.csv',index_col=0)\n",
    "dendromatnew=dendromat.loc[:,labelListnew]\n",
    "dendromatnew=dendromatnew.loc[labelListnew,:]\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "\n",
    "mask = np.triu(np.ones_like(dendromatnew, dtype=bool))\n",
    "\n",
    "sns.heatmap(dendromatnew,cmap='viridis',xticklabels=labelListnew,yticklabels=labelListnew,\n",
    "           cbar_kws={'label': 'Tree distance'},mask=mask,annot=True,ax=ax)\n",
    "\n",
    "plt.savefig('rfdistnew.pdf',bbox_inches='tight'),plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatter plots of correlation between matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_NetNoiseMat=NetNoiseMat[:,:20]\n",
    "tmp_NetNoiseMat=tmp_NetNoiseMat[:,::-1]\n",
    "\n",
    "myMatList=[]\n",
    "myMatList.append(tmp_NetNoiseMat)\n",
    "\n",
    "for file in os.listdir('./DATA/Matrici_Edo/'):\n",
    "    mydf=pd.read_csv('./DATA/Matrici_Edo/'+file,skiprows=1,index_col=0,header=None)\n",
    "    mydf.index=newindex\n",
    "    mydf=mydf.reindex(netList)\n",
    "    label=file.replace('.csv','')\n",
    "    label=label.replace('df_','')\n",
    "    \n",
    "    if label=='HubScore' or label=='Betweenness' or label=='Clustering_Coefficient':\n",
    "        myMatList.append(np.array(mydf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myproplist=['Normalized MSE from noise analysis','Hub score','Betweenness','Clustering coefficient']\n",
    "fig,ax=plt.subplots(2,3,figsize=(10,5))\n",
    "pear=pd.read_csv('pearson_logmatrix.csv',index_col=0)\n",
    "k=0\n",
    "l=0\n",
    "for i in range(len(myMatList)):\n",
    "    for j in range(len(myMatList)):\n",
    "        if i!=j and j>i:\n",
    "            print(i,j,k,l)\n",
    "\n",
    "            ax[k,l].set_xlabel(myproplist[i])\n",
    "            ax[k,l].set_ylabel(myproplist[j])\n",
    "            ax[k,l].set_title(r'$\\rho=%.2f$' % (pear.iloc[i,j]))\n",
    "            ax[k,l].scatter(np.sign(myMatList[i])*np.log(1.0+np.abs(myMatList[i])),\n",
    "                                                         np.sign(myMatList[j])*np.log(1.0+np.abs(myMatList[j])))\n",
    "            l+=1\n",
    "            if l==3:\n",
    "                k=1\n",
    "                l=0\n",
    "fig.tight_layout()\n",
    "plt.savefig('cmp_matrices.jpg',bbox_inches='tight'),plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heatmaps of MSE matrices ranked by MSE mean or CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,2,figsize=(10,4))\n",
    "xticks = ['0','0.1','0.2','0.3','0.4','0.5','0.6','0.7','0.8','0.9','mean']\n",
    "\n",
    "for i in range(len(netList)):\n",
    "    if netList[i]=='Oscillating_nr10_na10':\n",
    "        sns.heatmap(rankedmeanMSElist[i].drop(columns='cvMSE'),cmap='viridis',ax=ax[0],\n",
    "                xticklabels=xticks,yticklabels=rankedmeanMSElist[i].index,\n",
    "                cbar_kws={'label': 'Mean Squared Error'})\n",
    "        ax[0].set_title('MSE '+netList[i])\n",
    "    if netList[i]=='MasterRegulator':\n",
    "        sns.heatmap(rankedmeanMSElist[i].drop(columns='cvMSE'),cmap='viridis',ax=ax[1],\n",
    "                xticklabels=xticks,yticklabels=rankedmeanMSElist[i].index,\n",
    "                cbar_kws={'label': 'Mean Squared Error'})\n",
    "        ax[1].set_title('MSE '+netList[i])\n",
    "\n",
    "\n",
    "ax[0].set_xlabel(r'$\\sigma$')\n",
    "ax[1].set_xlabel(r'$\\sigma$')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('Noise_matrices.pdf'),plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hopreg-venv",
   "language": "python",
   "name": "hopreg-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
